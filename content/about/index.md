---
title: About
type: landing

sections:
  - block: markdown
    content:
      title: About KVCache.AI
      text: |
        ## Our Mission

        KVCache.AI is dedicated to advancing the state-of-the-art in Large Language Model (LLM) inference optimization. We focus on developing innovative solutions for efficient KVCache management, disaggregated architectures, and high-performance serving systems.

        Our open-source projects and research aim to make LLM deployment more accessible, efficient, and cost-effective for organizations of all sizes.

        ## What We Do

        - **Research & Development**: Pushing the boundaries of LLM inference optimization
        - **Open Source Projects**: Building tools and frameworks for the community
        - **Knowledge Sharing**: Publishing articles, tutorials, and best practices
        - **Collaboration**: Working with researchers and practitioners worldwide
    design:
      columns: '1'
      spacing:
        padding: ['40px', '0', '40px', '0']

  - block: markdown
    content:
      title: Directors and Officers
      text: |
        ### Leadership Team

        Our leadership team brings together expertise in distributed systems, machine learning, and high-performance computing.

        - **Dr. ZHANG Mingxing** - Co-Founder & Technical Director
          - Specializes in distributed systems and LLM serving architectures
          - Lead architect of Mooncake disaggregated architecture

        - **Shaoyuan Chen** - Co-Founder & Research Director
          - Expert in attention mechanisms and operator optimization
          - Lead researcher on MLA optimization and KVCache compression

        - **Admin** - Operations Director
          - Manages community engagement and project coordination
          - Oversees open-source initiatives and partnerships
    design:
      columns: '1'
      spacing:
        padding: ['40px', '0', '40px', '0']

  - block: markdown
    content:
      title: Members
      text: |
        ### Core Contributors

        Our team consists of passionate researchers, engineers, and open-source contributors dedicated to advancing LLM inference technology.

        #### Research Team
        - Focus on novel attention mechanisms (MLA, GQA, MQA)
        - KVCache compression and optimization techniques
        - Distributed serving architectures

        #### Engineering Team
        - High-performance CUDA kernel development
        - Framework integration and tooling
        - Performance profiling and benchmarking

        #### Community Team
        - Documentation and tutorials
        - Community support and engagement
        - Educational content creation

        ### Join Us

        We're always looking for talented individuals who share our passion for LLM optimization. Whether you're a researcher, engineer, or enthusiast, there are many ways to contribute to our projects.

        - Contribute to our [open-source projects](https://github.com/kvcache-ai)
        - Join discussions and share your ideas
        - Help improve documentation and tutorials
        - Collaborate on research and publications
    design:
      columns: '1'
      spacing:
        padding: ['40px', '0', '40px', '0']

  - block: markdown
    content:
      title: Team Photo
      text: |
        <div class="team-photo-container" style="max-width: 800px; margin: 0 auto; text-align: center;">
          <img src="/media/icon_1.png" alt="KVCache.AI Team" style="width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
          <p style="margin-top: 1rem; font-style: italic; color: #666;">The KVCache.AI Team</p>
        </div>

        <!--
        To update the team photo:
        1. Place your photo in the same directory as this file (content/about/)
        2. Name it "team-photo.jpg" or update the src attribute above
        3. Supported formats: jpg, png, webp
        -->
    design:
      columns: '1'
      spacing:
        padding: ['40px', '0', '60px', '0']
---
